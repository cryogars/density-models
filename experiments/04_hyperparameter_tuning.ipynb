{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from snowmodels.utils import DefaultTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_splits.pkl', 'rb') as f:\n",
    "    data_splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Hyperparameters\n",
    "\n",
    "We'll start by running all four models using their default hyperparamaters. This is essential to see if there is any performance gains from tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_splits['X_train']\n",
    "X_val = data_splits['X_val']\n",
    "y_train = data_splits['y_train']\n",
    "y_val = data_splits['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running baseline models with default parameters...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1905792, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 0.302447\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's rmse: 0.0483899\tvalid's rmse: 0.0488552\n",
      "\n",
      "Results with onehot encoder:\n",
      "LightGBM - R²: 0.7199, RMSE: 0.0489, Best iteration: 1000\n",
      "XGBoost - R²: 0.7362, RMSE: 0.0474, Best iteration: 999\n",
      "ExtraTrees - R²: 0.6734, RMSE: 0.0528\n",
      "RandomForest - R²: 0.7430, RMSE: 0.0468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 981\n",
      "[LightGBM] [Info] Number of data points in the train set: 1905792, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.302447\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's rmse: 0.0485449\tvalid's rmse: 0.049003\n",
      "\n",
      "Results with catboost encoder:\n",
      "LightGBM - R²: 0.7182, RMSE: 0.0490, Best iteration: 1000\n",
      "XGBoost - R²: 0.7330, RMSE: 0.0477, Best iteration: 999\n",
      "ExtraTrees - R²: 0.7217, RMSE: 0.0487\n",
      "RandomForest - R²: 0.7621, RMSE: 0.0450\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 733\n",
      "[LightGBM] [Info] Number of data points in the train set: 1905792, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.302447\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's rmse: 0.0484609\tvalid's rmse: 0.048945\n",
      "\n",
      "Results with target encoder:\n",
      "LightGBM - R²: 0.7189, RMSE: 0.0489, Best iteration: 1000\n",
      "XGBoost - R²: 0.7359, RMSE: 0.0474, Best iteration: 999\n",
      "ExtraTrees - R²: 0.6747, RMSE: 0.0527\n",
      "RandomForest - R²: 0.7431, RMSE: 0.0468\n",
      "\n",
      "Best model configuration:\n",
      "Model: random_forest_catboost\n",
      "R² score: 0.7621\n",
      "RMSE: 0.0450\n"
     ]
    }
   ],
   "source": [
    "default_tuner = DefaultTuner(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val\n",
    "    )\n",
    "\n",
    "baseline_results = default_tuner.run_default_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Optimization\n",
    "\n",
    "The purpose of this optimization is to have an idea of how far I can go with tree addition to Random Forest and Extra Trees before running out of memory using their respective default configurations. This will allow me know the number of trees to set in optuna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_density_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
